# ðŸš€ LangSwarm Handbook

**Welcome to the new LangSwarm!** This guide shows you how to build powerful multi-agent AI systems in minutes, not hours.

<!--
## Handbook Outline
1.  Introduction to LangSwarm
    -   What is LangSwarm?
    -   Core Concepts
2.  Getting Started
    -   Installation
    -   Your First LangSwarm Agent
3.  Core Features
    -   Configuration
    -   Agents
    -   Tools
    -   Memory
### Sessions

LangSwarm provides a powerful session management system that allows you to control how memory and context are shared between agents.

**Key Session Settings:**

-   **`unified_memory`**: (boolean, default: `false`) When `true`, enables shared conversation context across all agents in the session. This is the key to enabling collaborative multi-agent workflows.
-   **`scope`**: (string, default: `workflow`) Defines the boundary of memory sharing.
    -   `workflow`: Memory is isolated to a single workflow execution.
    -   `user`: Memory is shared across all workflows for a specific user.
    -   `global`: Memory is shared across all users and workflows (e.g., for a company-wide knowledge base).
-   **`sharing_strategy`**: (string, default: `all`) Controls how much context is shared between agents.
    -   `all`: Each agent sees the complete session history.
    -   `sequential`: Each agent sees only the output of the immediately preceding agent. This is great for linear pipelines and can significantly reduce token costs.
    -   `selective`: Agents see only relevant context based on smart criteria.
-   **`context_window_management`**: (string, default: `auto`) Manages the context window to prevent token overflow.
    -   `auto`: Automatically truncates the oldest messages when approaching the model's token limit.
    -   `smart_truncate`: Intelligently removes the least relevant context.
    -   `summarize`: Compresses older parts of the conversation into summaries.

**Example Session Configuration:**
```yaml
session:
  unified_memory: true
  scope: "user"
  sharing_strategy: "sequential"
  context_window_management: "smart_truncate"
  session_timeout: 3600 # 1 hour
```

### Response Modes

When an agent provides both a user response AND a tool call in the same JSON structure, LangSwarm supports two different modes for handling this situation: `"integrated"` (the default) and `"streaming"`.

**`"integrated"` Mode (Default):**
The user response is combined with tool results before being shown to the user. This is ideal for APIs and batch processing where a single, concise response is desired.

**`"streaming"` Mode:**
The user sees the agent's immediate response first, then the tool results are shown separately. This is great for chat interfaces and interactive tools where transparency and a conversational flow are important.

**Configuration:**
```yaml
agents:
  - id: my_agent
    response_mode: "streaming"
```
4.  Advanced Topics
    -   MCP (Multi-Controller Pattern)
    -   Zero-Config Customization
5.  Integration Guide
6.  Migration Guide
7.  Examples
-->

---

## ðŸŽ¯ **Quick Start - Your First Agent in 30 Seconds**

### **Option 1: Single Configuration File**
Create `langswarm.yaml`:
```yaml
version: "1.0"
agents:
  - id: "assistant"
    model: "gpt-4o"
    behavior: "helpful"
    memory: true
workflows:
  - "assistant -> user"
```

**Run it:**
```bash
python -c "
from langswarm.core.config import LangSwarmConfigLoader
loader = LangSwarmConfigLoader('langswarm.yaml')
workflows, agents, tools, brokers, metadata = loader.load()
print('âœ… Your agent is ready!')
"
```

### **Option 2: Pure Python API**
```python
from langswarm.core.agents.simple import create_chat_agent

# One line to create a complete agent
agent = create_chat_agent("assistant", memory_enabled=True)

# Start chatting immediately
response = agent.chat("Hello! Can you help me with some questions?")
print(response)

# Clean up when done
agent.cleanup()
```

**That's it!** You now have a fully functional AI agent with memory, chat capabilities, and professional-grade architecture.

---

## ðŸ§  **Memory Made Simple**

**Before (Complex):**
```yaml
memory:
  enabled: true
  backend: "chromadb"
  settings:
    persist_directory: "/path/to/storage"
    collection_name: "my_collection"
    embedding_function: "sentence-transformers"
    embedding_model: "all-MiniLM-L6-v2"
    chunk_size: 1000
    chunk_overlap: 200
    # ... 15+ more configuration options
```

**After (Simple):**
```yaml
# Tier 1: Development (instant setup)
memory: true

# Tier 2: Production (smart auto-configuration)  
memory: production

# Tier 3: Custom (when you need control)
memory:
  backend: chromadb
  settings: {persist_directory: "/custom/path"}
```

### **Smart Environment Detection**

LangSwarm automatically detects your environment and selects the optimal backend:

- **Google Cloud** â†’ BigQuery (analytics-ready, unlimited scale)
- **AWS** â†’ Elasticsearch (full-text search, AWS-native)
- **Redis Available** â†’ Redis (ultra-fast access, proven reliability)
- **Local/Development** â†’ SQLite (zero-config, instant start)
- **Production Fallback** â†’ ChromaDB (vector search, self-contained)

### Session-Scoped Unified Memory

Session-Scoped Unified Memory provides **automatic context sharing** between agents within a defined scope, creating seamless collaboration.

**How It Works**
```yaml
# langswarm.yaml
session:
  unified_memory: true
  scope: "workflow"
  sharing_strategy: "all"
```

**Key Characteristics**
- **Automatic Context Sharing**: Agents see the full conversation history.
- **Natural Collaboration**: Agents can build on each other's work without complex setup.
- **Context Window Management**: Automatic handling of token limits to prevent overflow.

### **Memory Usage Examples**

```python
# Development - instant SQLite setup
config = {"memory": True}

# Production - auto-detects best backend
config = {"memory": "production"}

# Custom - full control when needed
config = {
    "memory": {
        "backend": "redis",
        "settings": {"redis_url": "redis://localhost:6379"}
    }
}
```

---

## ðŸ”„ **Workflow Simplification**

**Before (Complex YAML):**
```yaml
workflows:
  - id: analysis_workflow
    steps:
      - id: step1
        agent: data_extractor
        input: ${context.user_input}
        output:
          to_step: step2
          format: extracted_data
      - id: step2
        agent: data_analyzer  
        input: ${step1.extracted_data}
        output:
          to_step: step3
          format: analysis_results
      - id: step3
        agent: report_generator
        input: ${step2.analysis_results}
        output:
          to: user
          format: final_report
```

**After (Simple Syntax):**
```yaml
workflows:
  - "extractor -> analyzer -> generator -> user"
```

### **Workflow Patterns Library**

**Linear Workflows:**
```yaml
workflows:
  - "assistant -> user"                    # Simple chat
  - "researcher -> writer -> user"         # Research and write
  - "analyzer -> summarizer -> user"       # Analyze and summarize
```

**Parallel Processing:**
```yaml
workflows:
  - "expert1, expert2, expert3 -> consensus -> user"   # Expert consensus
  - "fact_checker, editor -> publisher -> user"        # Content pipeline
```

**Conditional Routing:**
```yaml
workflows:
  - "router -> (technical_expert | business_expert) -> user"  # Smart routing
```

**Complex Multi-Stage:**
```yaml
workflows:
  - "intake -> analyzer -> (simple_response | expert1, expert2 -> consensus) -> formatter -> user"
```

### **Template Library (Copy & Paste Ready)**

```yaml
# Customer Support Pipeline
workflows:
  - "classifier -> (faq_bot | human_escalation) -> user"

# Content Creation Pipeline  
workflows:
  - "researcher -> drafter -> editor -> publisher -> user"

# Data Analysis Pipeline
workflows:
  - "extractor -> cleaner -> analyzer -> visualizer -> user"

# Code Review Pipeline
workflows:
  - "security_check, performance_check, style_check -> consensus -> user"

# Research Pipeline
workflows:
  - "searcher -> evaluator -> synthesizer -> reviewer -> user"
```

---

## ðŸ¤– **Simplified Agent Architecture**

### **Before (Complex Constructor):**
```python
from langswarm.core.wrappers.generic import AgentWrapper

# 22+ parameters to configure
agent = AgentWrapper(
    name="my_agent",
    agent=base_agent,
    model="gpt-4o",
    memory=memory_config,
    agent_type="conversational", 
    is_conversational=True,
    langsmith_api_key=api_key,
    rag_registry=rag_registry,
    context_limit=4000,
    system_prompt=system_prompt,
    tool_registry=tool_registry,
    plugin_registry=plugin_registry,
    memory_adapter=memory_adapter,
    memory_summary_adapter=summary_adapter,
    broker=message_broker,
    response_mode="structured",
    streaming_config=streaming_config,
    session_manager=session_manager,
    enable_hybrid_sessions=True,
    enhanced_backend=enhanced_backend,
    enhanced_config=enhanced_config,
    allow_middleware=True,
    # ... and more
)
```

### **After (Simple Configuration):**
```python
from langswarm.core.agents.simple import AgentConfig, SimpleAgent

# Single configuration object
config = AgentConfig(
    id="my_agent",
    model="gpt-4o", 
    behavior="helpful",
    memory_enabled=True,
    streaming_enabled=True,
    tools=["filesystem", "github"]
)

agent = SimpleAgent(config)
```

### **Factory Functions for Common Use Cases:**

```python
from langswarm.core.agents.simple import (
    create_chat_agent,
    create_coding_agent,
    create_research_agent
)

# Instant agent creation
chat_agent = create_chat_agent("assistant")
coding_agent = create_coding_agent("coder", tools=["filesystem", "github"])
research_agent = create_research_agent("researcher", memory_enabled=True)
```

### **Clean Agent API:**

```python
# Simple chat interface
response = agent.chat("Analyze this data for me")

# Streaming responses
for chunk in agent.chat_stream("Write a detailed report"):
    print(chunk, end="")

# Agent information
info = agent.get_info()
print(f"Agent: {info['id']}, Model: {info['model']}, Behavior: {info['behavior']}")

# Conversation management
agent.reset_conversation()
history = agent.conversation_history

# Memory operations
agent._store_memory("project_context", "Working on AI simplification")
context = agent._retrieve_memory("project_context")

# Cleanup
agent.cleanup()
```

---

## ðŸ“„ **Unified Configuration Examples**

### **Complete Real-World Example**

```yaml
# langswarm.yaml - Complete multi-agent system
version: "1.0"
project_name: "content-creation-pipeline"

# Simplified agent definitions
agents:
  - id: researcher
    model: gpt-4o
    behavior: research
    tools: [web_search, filesystem]
    
  - id: writer
    model: gpt-4o
    behavior: creative
    memory_enabled: true
    
  - id: editor
    model: gpt-4o
    behavior: analytical
    tools: [grammar_check, style_check]
    
  - id: publisher
    model: gpt-4o
    behavior: helpful
    tools: [cms_integration, social_media]

# Smart memory configuration
memory: production

# Simple workflow definitions
workflows:
  - id: content_pipeline
    workflow: "researcher -> writer -> editor -> publisher -> user"
    
  - id: quick_response
    simple: "writer -> user"
    
  - id: fact_check_pipeline
    workflow: "researcher, editor -> consensus -> user"

# Tool configurations (optional - auto-discovery available)
tools:
  - id: web_search
    type: web_search_tool
    description: "Search the web for current information"
    
  - id: filesystem
    type: mcpfilesystem
    description: "Read and write files locally"
    local_mode: true
```

### **Development vs Production Configurations**

**Development Setup (instant start):**
```yaml
version: "1.0"
agents:
  - id: dev_assistant
    model: gpt-4o
    behavior: helpful
memory: true                    # SQLite auto-configured
workflows:
  - "dev_assistant -> user"     # Simple workflow
```

**Production Setup (optimized):**
```yaml
version: "1.0"
project_name: "production-ai-system"

agents:
  - id: primary_agent
    model: gpt-4o
    behavior: helpful
    tools: [filesystem, database_connector]
    memory_enabled: true
    streaming_enabled: true
    
  - id: specialist_agent
    model: gpt-4o
    behavior: analytical
    tools: [data_analyzer, report_generator]

memory: production             # Auto-selects optimal backend

workflows:
  - id: main_flow
    workflow: "primary_agent -> specialist_agent -> user"
    
  - id: direct_response
    simple: "primary_agent -> user"

tools:
  - id: database_connector
    type: custom_db_tool
    settings:
      connection_string: ${ENV.DATABASE_URL}
      
  - id: data_analyzer
    type: analytics_tool
    settings:
      api_key: ${ENV.ANALYTICS_API_KEY}
```

---

## 5. Integration Guide

This section explains how to integrate LangSwarm's features, like the Intelligent Workflow Navigation system, into your existing codebase.

### Integration Strategy

LangSwarm features are designed for phased integration to ensure compatibility and a smooth transition.

1.  **Standalone Module**: Features are developed as independent modules with their own core functionality and tests.
2.  **Workflow Integration**: New features are integrated into the existing workflow execution engine, often by adding new step types while maintaining backward compatibility.
3.  **UI Integration**: New UI components, like dashboards and visualizations, are added to the existing UI.
4.  **Advanced Features**: A/B testing, machine learning optimization, and advanced analytics are added in later phases.

### Core Integration Points

-   **Workflow Step Type Registration**: New features often introduce new step types (e.g., a `navigation` step) that need to be registered in `langswarm/core/config.py`.
-   **Tool Registration**: New tools provided by features are registered in `langswarm/core/wrappers/generic.py`.
-   **Configuration Schema Updates**: The configuration schema in `langswarm/core/config.py` is updated to validate the new step types and their parameters.
-   **Database Integration**: New tables are added to the database to track feature-specific data, like navigation decisions, in `langswarm/core/session/models.py`.
-   **Analytics Integration**: New analytics hooks are added to track the performance and usage of new features.

### Backward Compatibility

All new features are designed to be fully backward compatible. Existing workflows and configurations will continue to work without any changes. New features are enabled through new configuration options, so you can adopt them at your own pace.

## 6. Migration Guide

This section helps existing LangSwarm users migrate from complex multi-file configurations to the new simplified system while preserving all functionality.

### Migration Overview

- **8 config files â†’ 1 unified file** (87.5% file reduction)
- **22+ agent parameters â†’ 1 config object** (95% parameter reduction)
- **15+ line workflows â†’ 1 line syntax** (90% complexity reduction)
- **20+ memory settings â†’ 3 simple tiers** (95% configuration reduction)

**You can migrate at your own pace - old and new syntax work side by side.**

### Quick Migration (5 Minutes)

1.  **Backup Your Current Configuration**: `cp -r your_langswarm_project your_langswarm_project_backup`
2.  **Create Simplified Configuration**: Create a single `langswarm.yaml` with the new, simplified syntax.
3.  **Test Migration**: Use `LangSwarmConfigLoader` to load the new config and ensure it works.
4.  **Update Your Code**: Switch from complex agent constructors to the new simple factory functions like `create_chat_agent`.

### Agent Configuration Migration

**Before (agents.yaml):**
```yaml
agents:
  - id: customer_support
    agent_type: langchain-openai
    model: gpt-4o
    system_prompt: "You are a helpful customer support agent..."
    # ... 15+ more parameters
```

**After (langswarm.yaml):**
```yaml
agents:
  - id: customer_support
    model: gpt-4o
    behavior: helpful
    tools: [filesystem, knowledge_base]
    memory_enabled: true
```

### Workflow Migration

**Before (workflows.yaml):**
```yaml
workflows:
  - id: support_escalation_workflow
    name: "Customer Support Escalation"
    steps:
      - id: initial_classification
        agent: classifier_agent
        # ... complex step definition
```

**After (langswarm.yaml):**
```yaml
workflows:
  - "classifier -> (faq_bot | specialist | expert) -> user"
```

### **Before & After Comparisons**

**1. Agent Creation**

Before:
```python
# 50+ lines of configuration
memory_config = MemoryConfig(
    enabled=True,
    backend="chromadb", 
    settings={...}
)

tool_registry = ToolRegistry()
# ... register tools ...

plugin_registry = PluginRegistry()  
# ... register plugins ...

agent = AgentWrapper(
    name="assistant",
    agent=base_agent,
    model="gpt-4o",
    memory=memory_config,
    tool_registry=tool_registry,
    plugin_registry=plugin_registry,
    # ... 15+ more parameters
)
```

After:
```python
# 1 line of code
agent = create_chat_agent("assistant", memory_enabled=True)
```

**2. Workflow Definition**

Before:
```yaml
# 30+ lines of YAML
workflows:
  - id: analysis_workflow
    steps:
      - id: extract_step
        agent: extractor
        input: ${context.user_input}
        output:
          to_step: analyze_step
          format: raw_data
      - id: analyze_step
        agent: analyzer
        input: ${extract_step.raw_data}
        output:
          to_step: summarize_step
          format: analysis_results
      - id: summarize_step
        agent: summarizer
        input: ${analyze_step.analysis_results}
        output:
          to: user
          format: final_summary
```

After:
```yaml
# 1 line of YAML
workflows:
  - "extractor -> analyzer -> summarizer -> user"
```

**3. Memory Configuration**

Before:
```yaml
# 20+ lines of configuration
memory:
  enabled: true
  backend: "chromadb"
  settings:
    persist_directory: "/data/vectorstore"
    collection_name: "agent_memory"
    embedding_function: "sentence-transformers"
    embedding_model: "all-MiniLM-L6-v2"
    distance_metric: "cosine"
    chunk_size: 1000
    chunk_overlap: 200
    max_tokens: 8192
    search_top_k: 5
    search_score_threshold: 0.7
    batch_size: 100
    # ... more settings
```

After:
```yaml
# 1 line of configuration
memory: production
```

---

## ðŸ“Š **Performance & Scalability**

### **Setup Time Comparison**

| Task | Before | After | Improvement |
|------|--------|-------|-------------|
| **Create first agent** | 2+ hours | 30 seconds | **240x faster** |
| **Add memory** | 1 hour | 1 line | **3600x faster** |
| **Define workflow** | 30 minutes | 1 line | **1800x faster** |
| **Production deployment** | 1 day | 5 minutes | **288x faster** |

### **Configuration Complexity**

| Component | Before | After | Reduction |
|-----------|--------|-------|-----------|
| **Agent parameters** | 22+ parameters | 1 config object | **95% reduction** |
| **Workflow lines** | 15+ lines YAML | 1 line syntax | **90% reduction** |
| **Memory config** | 20+ settings | 1 tier selection | **95% reduction** |
| **File count** | 8 separate files | 1 unified file | **87.5% reduction** |

### **Performance Benchmarks**

All simplification features maintain or exceed performance:

- **Memory operations**: <1ms per operation
- **Workflow generation**: <2ms per workflow  
- **Agent creation**: <10ms per agent
- **Configuration loading**: <2s for complex setups
- **End-to-end processing**: <5s for full systems

---

## ðŸŽ¯ **Use Case Examples**

### **1. Customer Support Bot**

```yaml
version: "1.0"
agents:
  - id: support_bot
    model: gpt-4o
    behavior: helpful
    tools: [knowledge_base, ticket_system]
    memory: production
workflows:
  - "support_bot -> user"
```

### **2. Content Creation Pipeline**

```yaml
version: "1.0"
agents:
  - {id: researcher, model: gpt-4o, behavior: research, tools: [web_search]}
  - {id: writer, model: gpt-4o, behavior: creative}
  - {id: editor, model: gpt-4o, behavior: analytical}
memory: production
workflows:
  - "researcher -> writer -> editor -> user"
```

### **3. Code Review System**

```yaml
version: "1.0"
agents:
  - {id: security_expert, model: gpt-4o, behavior: analytical, tools: [security_scanner]}
  - {id: performance_expert, model: gpt-4o, behavior: analytical, tools: [profiler]}
  - {id: style_expert, model: gpt-4o, behavior: helpful, tools: [linter]}
  - {id: consensus_agent, model: gpt-4o, behavior: analytical}
memory: production
workflows:
  - "security_expert, performance_expert, style_expert -> consensus_agent -> user"
```

### **4. Research Assistant**

```yaml
version: "1.0"
agents:
  - {id: searcher, model: gpt-4o, behavior: research, tools: [web_search, academic_db]}
  - {id: analyzer, model: gpt-4o, behavior: analytical, memory_enabled: true}
  - {id: synthesizer, model: gpt-4o, behavior: creative}
memory: production
workflows:
  - "searcher -> analyzer -> synthesizer -> user"
```

### **5. Data Processing Pipeline**

```yaml
version: "1.0"
agents:
  - {id: extractor, model: gpt-4o, behavior: analytical, tools: [data_sources]}
  - {id: cleaner, model: gpt-4o, behavior: helpful, tools: [data_validator]}
  - {id: analyzer, model: gpt-4o, behavior: analytical, tools: [statistics]}
  - {id: visualizer, model: gpt-4o, behavior: creative, tools: [charts]}
memory: production
workflows:
  - "extractor -> cleaner -> analyzer -> visualizer -> user"
```

---

## 4. Advanced Topics

### MCP (Multi-Controller Pattern)

**Local MCP Mode** enables zero-latency tool execution directly within LangSwarm without containers, external servers, or complex deployment. This revolutionary approach delivers 1000x faster tool calls while maintaining full MCP compatibility.

#### How Local MCP Works

```mermaid
graph LR
    A[Workflow YAML] --> B[WorkflowExecutor]
    B --> C[mcp_call function]
    C --> D{URL Type?}
    D -->|local://| E[Local Server Registry]
    D -->|http://| F[HTTP Request]
    D -->|stdio://| G[External Process]
    E --> H[Direct Function Call]
    H --> I[0ms Response]
```

#### Performance Comparison

| Mode | Latency | Setup | Use Case |
|------|---------|-------|----------|
| **Local Mode** | **0.0ms** | Zero setup | Development, simple tools |
| HTTP Mode | 50-100ms | Docker/uvicorn | Production deployment |
| Stdio Mode | 20-50ms | External process | Complex external tools |

#### Building Custom Local Tools

**Step 1: Create Tool Implementation**

```python
# my_tools/calculator.py
from langswarm.mcp.server_base import BaseMCPToolServer
from pydantic import BaseModel

class CalculateInput(BaseModel):
    expression: str

class CalculateOutput(BaseModel):
    result: float
    expression: str

def calculate_handler(expression: str):
    """Safely evaluate mathematical expressions"""
    try:
        # Use a safe evaluator in production
        result = eval(expression)  # Note: Use ast.literal_eval for safety
        return {"result": result, "expression": expression}
    except Exception as e:
        return {"result": 0, "expression": f"Error: {e}"}

# Create local MCP server
calculator_server = BaseMCPToolServer(
    name="calculator",
    description="Mathematical calculator tool",
    local_mode=True  # ðŸ”§ Enable local mode
)

calculator_server.add_task(
    name="calculate",
    description="Evaluate a mathematical expression",
    input_model=CalculateInput,
    output_model=CalculateOutput,
    handler=calculate_handler
)

# Auto-register when imported
app = calculator_server.build_app()
```

**Step 2: Use in Workflows**

```yaml
# workflows.yaml
workflows:
  main_workflow:
    - id: math_workflow
      steps:
        - id: calculate_result
          function: langswarm.core.utils.workflows.functions.mcp_call
          args:
            mcp_url: "local://calculator"  # Matches server name
            task: "calculate"
            params: {"expression": "2 + 2 * 3"}
          output:
            to: user
```

#### Remote MCP Tools

LangSwarm also supports connecting to remote MCP (Model-Compatible Protocol) servers through HTTP/HTTPS endpoints. This enables integration with external services, third-party tool providers, and cloud-hosted MCP servers.

**Basic Remote MCP Tool Configuration**

```yaml
# tools.yaml
tools:
  - id: remote_service
    type: mcpremote
    description: "Remote MCP service for data processing"
    mcp_url: "https://your-mcp-server.com/api"
    headers:
      Authorization: "Bearer ${API_TOKEN}"
    timeout: 30
    retry_count: 3
```

**Authentication**

Use environment variables for sensitive information like API keys and tokens.

```yaml
tools:
  - id: secure_service
    type: mcpremote
    mcp_url: "https://api.service.com/mcp"
    headers:
      x-api-key: "${SERVICE_API_KEY}"
```

### Zero-Config Customization

The zero-config system provides intelligent defaults, but you can easily customize them to match your specific needs.

#### Behavior Profiles

The primary way to customize is by editing **Behavior Profiles** in `langswarm/core/defaults.py`. These profiles define default settings for different agent behaviors like "coding", "research", etc.

```python
# langswarm/core/defaults.py
"coding": BehaviorProfile(
    name="Coding Assistant",
    preferred_models=["gpt-4o", "claude-3-5-sonnet-20241022"],
    max_tokens=8000,
    temperature=0.1,
    tools=["filesystem", "github", "codebase_indexer"],
),
```

You can add new profiles or modify existing ones to change the default model, temperature, tools, and more for a given behavior.

#### Configuration File Override

You can also override defaults directly in your `langswarm.yaml` file for per-agent customization without changing the global defaults.

```yaml
# langswarm.yaml
version: "1.0"
agents:
  - id: "custom-coder"
    behavior: "coding"
    model: "claude-3-5-sonnet-20241022"  # Override default model
    temperature: 0.05                    # Override default temperature
```

### **Environment-Specific Configurations**

### **Environment-Specific Configurations**

```yaml
# Development
version: "1.0"
agents:
  - id: dev_agent
    model: gpt-4o-mini      # Faster, cheaper for dev
    behavior: helpful
memory: true                # SQLite for dev
workflows:
  - "dev_agent -> user"

---
# Production  
version: "1.0"
agents:
  - id: prod_agent
    model: gpt-4o           # Full power for production
    behavior: helpful
    streaming_enabled: true
memory: production          # Auto-selects optimal backend
workflows:
  - "prod_agent -> user"
```

### **Tool Auto-Discovery**

```yaml
# No tools section needed - auto-discovers available tools
version: "1.0"
agents:
  - id: smart_agent
    model: gpt-4o
    behavior: coding
    # tools: [filesystem, github]  # Auto-discovered based on behavior!
memory: production
workflows:
  - "smart_agent -> user"
```

### **Behavior-Driven System Prompts**

```yaml
# Behaviors automatically generate appropriate system prompts
agents:
  - id: coding_expert
    behavior: coding        # Auto-generates coding-focused prompt
  - id: research_assistant  
    behavior: research      # Auto-generates research-focused prompt
  - id: creative_writer
    behavior: creative      # Auto-generates creative writing prompt
```

---

## ðŸŽ‰ **Success Stories**

### **Before LangSwarm Simplification:**
*"It took me 3 days just to get a basic agent working. I had to learn YAML syntax, understand 5 different configuration files, and figure out complex memory backends. I gave up twice before finally getting something running."*

### **After LangSwarm Simplification:**
*"I had a working multi-agent system in 10 minutes. The simple syntax is intuitive, memory 'just works', and I can focus on my business logic instead of configuration complexity. This is a game-changer!"*

---

## ðŸ“š **Next Steps**

### **Learn More:**
- [Memory Made Simple Guide](./simplification/04-memory-made-simple.md)
- [Workflow Simplification Examples](./simplification/workflow-examples.md)
- [Simplified Agent Architecture](./simplification/agent-architecture.md)

### **Get Help:**
- [GitHub Issues](https://github.com/langswarm/langswarm/issues)
- [Community Discord](https://discord.gg/langswarm)
- [Documentation](https://docs.langswarm.dev)

### **Contribute:**
- [Contributing Guide](./CONTRIBUTING.md)
- [Development Setup](./DEVELOPMENT.md)
- [Feature Requests](./FEATURE_REQUESTS.md)

---

**Welcome to the simplified LangSwarm! Build powerful AI systems with ease.** ðŸš€
