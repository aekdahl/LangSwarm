version: "1.0"
project_name: "message_queue_consumer_example"

# Configuration for LangSwarm as a distributed task processor
memory: "production"

tools:
  - id: task_consumer
    type: mcpmessage_queue_consumer
    description: "Message queue consumer for distributed task processing"

agents:
  # Main task consumer agent
  - id: task_processor
    agent_type: openai
    model: gpt-4o
    system_prompt: |
      You are a Task Processor Agent that manages message queue consumers for distributed task processing.
      
      Your responsibilities:
      1. Start consumers to poll message queues
      2. Monitor task processing performance
      3. Handle consumer lifecycle management
      4. Optimize processing based on workload
      
      **Available Operations:**
      - start_consumer: Begin consuming tasks from a queue
      - stop_consumer: Gracefully stop a consumer
      - list_consumers: Show all active consumers
      - get_consumer_stats: Monitor performance metrics
      - pause_consumer: Temporarily halt processing
      - resume_consumer: Resume paused consumer
      
      **Broker Types:**
      - redis: Fast, simple, good for general use
      - gcp_pubsub: Enterprise-grade with guaranteed delivery
      - in_memory: Development and testing
      
      **Performance Guidelines:**
      - Start with 3-5 max_workers for balanced performance
      - Use 1-2 second poll_interval for responsiveness
      - Set task_timeout based on expected processing time
      - Monitor success rates and adjust retry_attempts
      
      Always provide structured responses using the task_consumer tool.
    
    tools:
      - task_consumer

workflows:
  # Start basic consumer
  start_task_consumer:
    description: "Start a message queue consumer"
    steps:
      - agent: task_processor
        input: "${user_input}"
        output:
          to: user

  # Monitor consumer performance
  monitor_task_processing:
    description: "Monitor task processing performance and health"
    steps:
      - agent: task_processor
        input: "${user_input}"
        output:
          to: user

# Example usage scenarios:
# 1. "Start a Redis consumer for the 'tasks' queue with 5 workers"
# 2. "Show me the statistics for all running consumers"
# 3. "Pause the 'redis_worker_1' consumer for maintenance"
# 4. "Start a GCP Pub/Sub consumer for enterprise tasks"