# BigQuery Memory Backend Example Configuration
# ===========================================
# 
# This example shows different ways to configure BigQuery as your memory backend.
# Choose the approach that best fits your needs.

version: "1.0"

# =============================================================================
# OPTION 1: AUTO-DETECTION (RECOMMENDED)
# =============================================================================
# LangSwarm automatically detects BigQuery when you have:
# - GOOGLE_APPLICATION_CREDENTIALS environment variable set
# - GOOGLE_CLOUD_PROJECT environment variable set

agents:
  - id: "auto-bigquery-agent"
    model: "gpt-4o"
    behavior: "helpful"
    memory: production  # ← Automatically uses BigQuery if detected!

---

# =============================================================================
# OPTION 2: EXPLICIT BIGQUERY CONFIGURATION
# =============================================================================
# Explicitly specify BigQuery settings for full control

agents:
  - id: "explicit-bigquery-agent"
    model: "gpt-4o"
    behavior: "helpful"
    memory:
      backend: "bigquery"
      settings:
        project_id: "your-langswarm-project"  # Your Google Cloud project
        dataset_id: "langswarm_memory"        # Dataset name (auto-created)
        table_id: "agent_conversations"       # Table name (auto-created)
        location: "US"                        # Data location (US, EU, etc.)

---

# =============================================================================
# OPTION 3: ADVANCED CONFIGURATION
# =============================================================================
# Advanced BigQuery configuration with analytics features

agents:
  - id: "analytics-agent"
    model: "gpt-4o"
    behavior: "analytical"
    memory:
      backend: "bigquery"
      settings:
        project_id: "analytics-project-123"
        dataset_id: "ai_analytics"
        table_id: "conversation_data"
        location: "US"
        
        # Advanced features
        description: "Analytics-powered agent conversations"
        retention_days: 365
        clustering_fields: ["agent_id", "session_id"]
        partitioning: "timestamp"

  # Specialized analytics agent
  - id: "data-scientist-agent"
    model: "gpt-4o"
    behavior: "analytical"
    system_prompt: |
      You are a data science assistant with access to BigQuery analytics.
      You can analyze conversation patterns, user behavior, and agent performance.
      
      Available analytics capabilities:
      - Conversation volume analysis
      - Session pattern detection
      - Agent performance metrics
      - User engagement analysis
    memory:
      backend: "bigquery"
      settings:
        project_id: "analytics-project-123"
        dataset_id: "ai_analytics"
        table_id: "research_conversations"

---

# =============================================================================
# OPTION 4: MULTI-AGENT WITH SHARED BIGQUERY
# =============================================================================
# Multiple agents sharing the same BigQuery backend

agents:
  - id: "customer-support"
    model: "gpt-4o"
    behavior: "support"
    memory:
      backend: "bigquery"
      settings:
        project_id: "company-ai-platform"
        dataset_id: "customer_interactions"
        table_id: "support_conversations"

  - id: "sales-assistant"
    model: "gpt-4o"
    behavior: "helpful"
    memory:
      backend: "bigquery"
      settings:
        project_id: "company-ai-platform"
        dataset_id: "sales_interactions"
        table_id: "sales_conversations"

  - id: "product-advisor"
    model: "gpt-4o"
    behavior: "helpful"
    memory:
      backend: "bigquery"
      settings:
        project_id: "company-ai-platform"
        dataset_id: "product_interactions"
        table_id: "advisory_conversations"

---

# =============================================================================
# ENVIRONMENT VARIABLES SETUP
# =============================================================================
# Before using any of the above configurations, set these environment variables:
#
# export GOOGLE_APPLICATION_CREDENTIALS="/path/to/service-account.json"
# export GOOGLE_CLOUD_PROJECT="your-project-id"
#
# For auto-detection (Option 1), these are the only variables needed.
# For explicit configuration (Options 2-4), you can override with specific settings.

# =============================================================================
# BIGQUERY ANALYTICS EXAMPLES
# =============================================================================
# Once your agents are running, you can analyze the data with SQL queries:

# Query 1: Most active conversation sessions
# SELECT 
#   session_id,
#   agent_id,
#   COUNT(*) as message_count,
#   MIN(timestamp) as session_start,
#   MAX(timestamp) as session_end
# FROM `your-project.langswarm_memory.agent_conversations`
# WHERE timestamp >= TIMESTAMP_SUB(CURRENT_TIMESTAMP(), INTERVAL 7 DAY)
# GROUP BY session_id, agent_id
# ORDER BY message_count DESC
# LIMIT 20;

# Query 2: Daily conversation volume
# SELECT 
#   EXTRACT(DATE FROM timestamp) as date,
#   agent_id,
#   COUNT(*) as daily_messages,
#   COUNT(DISTINCT session_id) as daily_sessions
# FROM `your-project.langswarm_memory.agent_conversations`
# WHERE timestamp >= TIMESTAMP_SUB(CURRENT_TIMESTAMP(), INTERVAL 30 DAY)
# GROUP BY date, agent_id
# ORDER BY date DESC, agent_id;

# Query 3: Average response length by agent
# SELECT 
#   agent_id,
#   COUNT(*) as total_responses,
#   AVG(LENGTH(agent_response)) as avg_response_length,
#   MIN(LENGTH(agent_response)) as min_response_length,
#   MAX(LENGTH(agent_response)) as max_response_length
# FROM `your-project.langswarm_memory.agent_conversations`
# WHERE agent_response IS NOT NULL
# GROUP BY agent_id
# ORDER BY avg_response_length DESC;